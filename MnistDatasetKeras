from __future__ import print_function
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import PReLU
from keras.datasets import mnist
from keras import backend as K
from keras.callbacks import LearningRateScheduler
import numpy as np
from keras.utils import np_utils
import math

number_of_classes = 10
epochs = 10
# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)
test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

train_labels = np_utils.to_categorical(train_labels, number_of_classes)
test_labels = np_utils.to_categorical(test_labels, number_of_classes)

train_images = train_images.astype('float32')
test_images = test_images.astype('float32')
train_images /= 255
test_images /= 255

#Instantiate an empty model
model = Sequential()

# 1st Convolutional Layer
model.add(Conv2D(filters=64, input_shape=(28,28,1), kernel_size=(3,3), strides=(1,1), padding='same'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

# 2nd Convolutional Layer
model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same'))
model.add(Activation('relu'))
# Max Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'))

# 3rd Convolutional Layer
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same'))
model.add(Activation('relu'))

# Max Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(3,3), padding='same'))

# Passing it to a Fully Connected layer
model.add(Flatten())
# 1st Fully Connected Layer
model.add(Dense(512, input_shape=(28*28*1,)))
model.add(Activation('relu'))
model.add(Dropout(0.4))


# Output Layer
model.add(Dense(number_of_classes))
model.add(Activation('softmax'))

#model.summary()

# Compile the model
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.001), metrics=['accuracy'])

# Adjust The Learning Rate
def learning_rate_changes(epoch):
   initial_lrate = 0.1
   drop = 0.5
   epochs_drop = 3.0
   lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
   return lrate
lrate = LearningRateScheduler(learning_rate_changes)

callbacks_list = [lrate]

results = model.fit(train_images, train_labels,          
          epochs=epochs, callbacks =callbacks_list,
          verbose=1,
          validation_data=(test_images, test_labels))
score = model.evaluate(test_images, test_labels, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Display Plots
#show_plots(results.history)
